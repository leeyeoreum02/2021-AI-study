{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1/25 AI study.ipynb","provenance":[],"authorship_tag":"ABX9TyNx5UtoT4VAVj0jgSGARrYJ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"VG_bNRvZt8mI"},"source":["import torch\r\n","from torch import nn, Tensor\r\n","# import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","\r\n","\r\n","class Conv2dReLU(nn.Sequential):\r\n","    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\r\n","        _layer = [] # 층의 개수가 달라도 호환되도록\r\n","        for _ in range(num_layers):\r\n","            _layer += [\r\n","                       nn.Conv2d(in_channels, out_channels, kernel_size=3),\r\n","                       nn.ReLU\r\n","            ] # append 안씀 -> configs=cfgs\r\n","        super().__init__(*_layers)\r\n","\r\n","class VGG16(nn.Module):\r\n","    def __init__(self, in_channel=3):\r\n","        super(VGG16, self).__init__() # python 2.x -> python 3.x super().__init__()\r\n","        self.conv1 = Conv2dReLU(3, 64, kernel_size=3, padding=1)\r\n","\r\n","        self.conv2 = Conv2dReLU(64, 64, kernel_size=3, padding=1)\r\n","\r\n","        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\r\n","        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\r\n","        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\r\n","        \r\n","        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\r\n","        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\r\n","        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\r\n","\r\n","        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\r\n","        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\r\n","        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\r\n","\r\n","        # self.pool = nn.MaxPool2d(2, 2)\r\n","        self.avgpool = nn.AdaptiveAvgPool2d((7, 7)) \r\n","        # nn.AdaptiveAvgPool2d: 어떤 input_size가 들어와도 7*7로 output 고정 -> fc오류방지\r\n","        # nn.AdaptiveAvgPool2d는 fc input_size를 조정하므로 conv층이 끝나고 마지막에 넣기\r\n","        self.maxpool = nn.MaxPool2d(2, 2)\r\n","\r\n","        self.fc6 = nn.Linear(in_features=512*7*7, out_features=4096)\r\n","        self.fc6 = nn.Linear(in_features=4096, out_features=4096)\r\n","        self.fc6 = nn.Linear(in_features=4096, out_features=1000)\r\n","\r\n","    def forward(self, x):\r\n","        x = F.relu(self.conv1(x))\r\n","        x = F.relu(self.conv2(x))\r\n","        x = F.relu(self.conv3_1(x))\r\n","        x = F.relu(self.conv3_2(x))\r\n","        x = F.relu(self.conv3_3(x))\r\n","        x = self.maxpool(x)\r\n","        x = F.relu(self.conv4_1(x))\r\n","        x = F.relu(self.conv4_2(x))\r\n","        x = F.relu(self.conv4_3(x))\r\n","        x = self.maxpool(x)\r\n","        x = F.relu(self.conv5_1(x))\r\n","        x = F.relu(self.conv5_2(x))\r\n","        x = F.relu(self.conv5_3(x))\r\n","        x = self.avgpool(x)\r\n","\r\n","        x = x.view(-1, 7*7*512) # reshape\r\n","\r\n","        x = F.relu(self.fc6(x))\r\n","        x = F.relu(self.fc7(x))\r\n","        x = F.relu(self.fc8(x))\r\n","\r\n","        return x\r\n","\r\n","if __name__ == '__main__':\r\n","    from torchsummary import summary\r\n","\r\n","    model = VGG16()\r\n","    print(summary(model, input_data=(3, 224, 224), verbose=0))\r\n","\r\n","    from torchvision.models import vgg16\r\n","\r\n","    model = vgg16(pretrained=False)\r\n","    print(summary(model, input_data(3, 224, 224), verbose=0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zsCcNX1CyWtj"},"source":[""],"execution_count":null,"outputs":[]}]}